import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

# From Natalia's code
def download_stock_data(tickers):
    data = yf.download(tickers, start='2010-01-01', end='2023-01-01')
    return data['Adj Close']

tickers = ['AAPL', 'GOOGL', 'MSFT']
stock_data = download_stock_data(tickers)


def calculate_returns(data):
    returns = data.pct_change().dropna()
    return returns

stock_returns = calculate_returns(stock_data)

# Split that data
X = stock_returns.drop('AAPL', axis=1)
y = stock_returns['AAPL']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# baby decision trees I don't really care about yet
dt_model = DecisionTreeRegressor(random_state=55)
dt_model.fit(X_train, y_train)
y_pred = dt_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# Enhanced Neural Network setup
def build_complex_model(input_shape):
    model = Sequential([
        Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape),
        BatchNormalization(),
        Dropout(0.2),
        LSTM(64, return_sequences=True),
        Dropout(0.2),
        LSTM(32),
        Dropout(0.2),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

X_train_reshaped = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_reshaped = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))
input_shape = (X_train_reshaped.shape[1], X_train_reshaped.shape[2])

nn_model = build_complex_model(input_shape)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history = nn_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])

# Visualization of model training
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend()
plt.show()
